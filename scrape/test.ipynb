{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder1 = DoubleConv(3, 64)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.decoder3 = DoubleConv(512 + 256, 256)\n",
    "        self.decoder2 = DoubleConv(256 + 128, 128)\n",
    "        self.decoder1 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.decoder3(torch.cat([self.upsample(e4), e3], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([self.upsample(d3), e2], dim=1))\n",
    "        d1 = self.decoder1(torch.cat([self.upsample(d2), e1], dim=1))\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset and utilities\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "class CardDataset(Dataset):\n",
    "    def __init__(self, metadata_file, transform=None):\n",
    "        self.transform = transform\n",
    "        self.metadata = []\n",
    "        self.base_dir = os.path.dirname(metadata_file)\n",
    "        \n",
    "        # Read and filter metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            all_metadata = [json.loads(line) for line in f]\n",
    "        \n",
    "        print(\"\\nDebugging file paths:\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        \n",
    "        # Get list of available cropped images\n",
    "        exact_crop_dir = os.path.join(self.base_dir, 'training_data/exact_crop')\n",
    "        pictures_dir = os.path.join(self.base_dir, 'pictures')\n",
    "        \n",
    "        print(f\"\\nChecking directories:\")\n",
    "        print(f\"Pictures directory: {pictures_dir}\")\n",
    "        print(f\"Cropped images directory: {exact_crop_dir}\")\n",
    "        \n",
    "        try:\n",
    "            cropped_files = os.listdir(exact_crop_dir)\n",
    "            print(f\"\\nFound {len(cropped_files)} files in {exact_crop_dir}\")\n",
    "            print(\"First few cropped files:\", cropped_files[:5] if cropped_files else \"None\")\n",
    "            \n",
    "            original_files = os.listdir(pictures_dir)\n",
    "            print(f\"Found {len(original_files)} files in {pictures_dir}\")\n",
    "            print(\"First few original files:\", original_files[:5] if original_files else \"None\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing directories: {str(e)}\")\n",
    "            cropped_files = []\n",
    "            original_files = []\n",
    "        \n",
    "        # Create mapping of cert numbers to cropped images\n",
    "        cropped_images = {}\n",
    "        for f in cropped_files:\n",
    "            if f.startswith('cropped_cert_'):\n",
    "                original_name = f.replace('cropped_cert_', 'cert_')\n",
    "                cropped_images[original_name] = os.path.join(exact_crop_dir, f)\n",
    "        \n",
    "        # Filter for only valid entries where both original and cropped images exist\n",
    "        for data in all_metadata:\n",
    "            # Convert relative paths to absolute\n",
    "            original_path = os.path.join(self.base_dir, data.get('original_image', ''))\n",
    "            if not original_path:\n",
    "                continue\n",
    "                \n",
    "            # Get the filename without path\n",
    "            original_filename = os.path.basename(original_path)\n",
    "            \n",
    "            # Check if we have a matching cropped image\n",
    "            if original_filename not in cropped_images:\n",
    "                continue\n",
    "                \n",
    "            # Ensure both images exist\n",
    "            if not os.path.exists(original_path):\n",
    "                continue\n",
    "                \n",
    "            # Try to read both images to verify they're valid\n",
    "            try:\n",
    "                img = cv2.imread(original_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                    \n",
    "                # Create mask from points to verify points are valid\n",
    "                mask = self.create_mask(data['points'], img.shape)\n",
    "                if mask is None:\n",
    "                    continue\n",
    "                \n",
    "                # Store absolute path in metadata\n",
    "                data['original_image'] = original_path\n",
    "                self.metadata.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {original_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nFinal results:\")\n",
    "        print(f\"Found {len(self.metadata)} valid training samples out of {len(all_metadata)} total samples\")\n",
    "    \n",
    "    def create_mask(self, points, img_shape):\n",
    "        try:\n",
    "            mask = np.zeros(img_shape[:2], dtype=np.float32)\n",
    "            points = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(mask, [points], 1)\n",
    "            return mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating mask: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.metadata[idx]\n",
    "        \n",
    "        # Read image\n",
    "        image = cv2.imread(data['original_image'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create mask from points\n",
    "        mask = self.create_mask(data['points'], image.shape)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        \n",
    "        return image, mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training and validation functions\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iou = 0\n",
    "    num_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_iou += calculate_iou(outputs, masks).item()\n",
    "    \n",
    "    avg_loss = val_loss / num_batches\n",
    "    avg_iou = val_iou / num_batches\n",
    "    \n",
    "    return avg_loss, avg_iou\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50):\n",
    "    history = {'train_loss': [], 'train_iou': [], 'val_loss': [], 'val_iou': []}\n",
    "    best_val_iou = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        \n",
    "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_iou += calculate_iou(outputs, masks).item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_iou = train_iou / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_iou = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_iou'].append(avg_train_iou)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Training   - Loss: {avg_train_loss:.4f}, IoU: {avg_train_iou:.4f}')\n",
    "        print(f'  Validation - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_iou': val_iou,\n",
    "            }, 'best_model.pth')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualization functions\n",
    "def denormalize_image(image: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert normalized tensor back to numpy image.\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    image = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return (image * 255).astype(np.uint8)\n",
    "\n",
    "def visualize_predictions(model: nn.Module, \n",
    "                         val_dataset: Dataset, \n",
    "                         device: torch.device,\n",
    "                         num_samples: int = 5,\n",
    "                         output_dir: str = 'validation_results') -> None:\n",
    "    \"\"\"Visualize model predictions on validation samples.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a small subset of validation data\n",
    "    indices = np.random.choice(len(val_dataset), min(num_samples, len(val_dataset)), replace=False)\n",
    "    subset = Subset(val_dataset, indices)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (image, mask) in enumerate(subset):\n",
    "            # Add batch dimension\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            mask = mask.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = model(image)\n",
    "            \n",
    "            # Convert tensors to numpy arrays\n",
    "            image = denormalize_image(image[0])\n",
    "            mask = mask[0, 0].cpu().numpy()\n",
    "            pred = (pred[0, 0].cpu().numpy() > 0.5).astype(np.float32)\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(131)\n",
    "            plt.imshow(image)\n",
    "            plt.title('Original Image')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(132)\n",
    "            plt.imshow(mask, cmap='gray')\n",
    "            plt.title('Ground Truth Mask')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(133)\n",
    "            plt.imshow(pred, cmap='gray')\n",
    "            plt.title('Predicted Mask')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'prediction_{idx}.png'))\n",
    "            plt.show()\n",
    "            \n",
    "            # Also show the masked image\n",
    "            masked_pred = image.copy()\n",
    "            masked_pred[pred == 0] = 0\n",
    "            \n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(masked_pred)\n",
    "            plt.title('Masked Prediction')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f'masked_{idx}.png'))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup and training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define transforms\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = CardDataset('scrape/crop_metadata.jsonl', transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Dataset split: {train_size} training samples, {val_size} validation samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model and training\n",
    "model = UNet().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history['train_iou'], label='Train')\n",
    "plt.plot(history['val_iou'], label='Validation')\n",
    "plt.title('IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize predictions\n",
    "print(\"Generating validation visualizations...\")\n",
    "visualize_predictions(model, val_dataset, device, num_samples=5)\n",
    "print(\"Validation visualizations saved in 'validation_results' directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

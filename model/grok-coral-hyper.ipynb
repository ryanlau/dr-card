{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Load and Preprocess Dataset\n",
    "image_dir = '../grade_comparisons/'  # Update this path as needed\n",
    "\n",
    "# List all JPEG images in the directory\n",
    "file_names = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f)) and f.lower().endswith('.jpg')]\n",
    "\n",
    "def extract_grade(filename):\n",
    "    pattern = r'cropped_(\\d+)_cert'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        print(f\"no match for {filename}\")\n",
    "        return None\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'filename': file_names})\n",
    "df['grade'] = df['filename'].apply(extract_grade)\n",
    "df['full_path'] = df['filename'].apply(lambda x: os.path.join(image_dir, x))\n",
    "\n",
    "# Filter out missing images\n",
    "df = df[df['full_path'].apply(os.path.exists)]\n",
    "print(f\"Dataset size after filtering: {len(df)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Unique grades in full dataset:\", df['grade'].unique())\n",
    "print(\"Number of unique grades in full dataset:\", df['grade'].nunique())\n",
    "grade_counts = df['grade'].value_counts().reset_index()\n",
    "grade_counts.columns = ['grade', 'count']\n",
    "grade_counts['percent'] = grade_counts['count'] / len(df) * 100\n",
    "print(grade_counts)\n",
    "\n",
    "# Split into train and validation sets\n",
    "SPLIT_FRAC = 0.8\n",
    "train_df = df.sample(frac=SPLIT_FRAC, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "train_df['label'] = le.transform(train_df['grade'])\n",
    "val_df['label'] = le.transform(val_df['grade'])\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Cell 3: Define Transforms\n",
    "def pad_to_square(image, fill=0, padding_mode=\"constant\"):\n",
    "    w, h = image.size\n",
    "    max_wh = max(w, h)\n",
    "    pad_left = (max_wh - w) // 2\n",
    "    pad_top = (max_wh - h) // 2\n",
    "    pad_right = max_wh - w - pad_left\n",
    "    pad_bottom = max_wh - h - pad_top\n",
    "    return transforms.Pad((pad_left, pad_top, pad_right, pad_bottom), fill=fill, padding_mode=padding_mode)(image)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: pad_to_square(img, fill=0)),\n",
    "    transforms.Resize(224),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: pad_to_square(img, fill=0)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Cell 4: Create Dataset and DataLoader\n",
    "class CardDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['full_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = CardDataset(train_df, transform=train_transform)\n",
    "val_dataset = CardDataset(val_df, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(\"Data loaders created\")\n",
    "\n",
    "# Cell 5: Define CORAL Head and Modify Model\n",
    "class CoralHead(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes - 1)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Cell 6: Define CORAL Loss Function\n",
    "def coral_loss(logits, levels, class_weights=None):\n",
    "    batch_size = logits.size(0)\n",
    "    levels = levels.view(-1, 1).to(device)\n",
    "    targets = (levels > torch.arange(num_classes - 1).to(device)).float()\n",
    "    loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    if class_weights is not None:\n",
    "        sample_weights = class_weights[levels.squeeze()].to(device)\n",
    "        loss = loss * sample_weights.view(-1, 1)\n",
    "    return loss.mean()\n",
    "\n",
    "# Cell 7: Define Validation Function\n",
    "def validate_coral(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).int()\n",
    "            pred_levels = torch.sum(preds, dim=1).cpu().numpy()\n",
    "            true_levels = labels.cpu().numpy()\n",
    "            correct += np.sum(pred_levels == true_levels)\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Cell 8: Define Training Function with Save Path\n",
    "def train_model_coral(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, phase='initial', save_path='best_model.pth'):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    scaler = GradScaler()  # Updated for future compatibility\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():  # Updated for future compatibility\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        val_loss, val_acc = validate_coral(model, val_loader, criterion)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        print(f'{phase} Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Cell 9: Hyperparameter Tuning Loop\n",
    "import itertools\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "lr_initial_list = [1e-3, 5e-4]\n",
    "lr_fine_tune_list = [1e-4, 5e-5]\n",
    "activate_weights_list = [0, 1]\n",
    "dropout_prob_list = [0.0, 0.3]\n",
    "patience_list = [10, 15]\n",
    "epochs_list = [50]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(itertools.product(\n",
    "    lr_initial_list, lr_fine_tune_list, activate_weights_list, \n",
    "    dropout_prob_list, patience_list, epochs_list\n",
    "))\n",
    "\n",
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "WEIGHT_DECAY = 1e-4  # Fixed weight decay\n",
    "\n",
    "for idx, (lr_initial, lr_fine_tune, activate_weights, dropout_prob, patience, epochs) in enumerate(combinations):\n",
    "    print(f\"\\nTraining combination {idx+1}/{len(combinations)}: \"\n",
    "          f\"LR_INITIAL={lr_initial}, LR_FINE_TUNE={lr_fine_tune}, \"\n",
    "          f\"ACTIVATE_WEIGHTS={activate_weights}, DROPOUT_PROB={dropout_prob}, \"\n",
    "          f\"PATIENCE={patience}, EPOCHS={epochs}\")\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    LR_INITIAL = lr_initial\n",
    "    LR_FINE_TUNE = lr_fine_tune\n",
    "    ACTIVATE_WEIGHTS_TENSOR = activate_weights\n",
    "    DROPOUT_PROB = dropout_prob\n",
    "    PATIENCE = patience\n",
    "    EPOCHS_INITIAL = epochs\n",
    "    EPOCHS_FINE_TUNE = epochs\n",
    "    \n",
    "    # Create a fresh model instance with the specified dropout probability\n",
    "    model = timm.create_model('convnext_base', pretrained=True, drop_rate=DROPOUT_PROB)\n",
    "    in_features = model.head.fc.in_features\n",
    "    model.head.fc = CoralHead(in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Compute class weights if enabled\n",
    "    if ACTIVATE_WEIGHTS_TENSOR:\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    else:\n",
    "        class_weights_tensor = None\n",
    "    \n",
    "    # Initial training (train only the head)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'head' not in name:\n",
    "            param.requires_grad = False\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=LR_INITIAL, \n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_INITIAL, eta_min=1e-6)\n",
    "    history_initial = train_model_coral(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        lambda logits, labels: coral_loss(logits, labels, class_weights_tensor),\n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        EPOCHS_INITIAL, \n",
    "        'Initial',\n",
    "        save_path=f'best_model_initial_{idx}.pth'\n",
    "    )\n",
    "    \n",
    "    # Load the best model from initial training\n",
    "    model.load_state_dict(torch.load(f'best_model_initial_{idx}.pth'))\n",
    "    \n",
    "    # Fine-tuning (train all layers)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=LR_FINE_TUNE, \n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FINE_TUNE, eta_min=1e-6)\n",
    "    history_fine = train_model_coral(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        lambda logits, labels: coral_loss(logits, labels, class_weights_tensor),\n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        EPOCHS_FINE_TUNE, \n",
    "        'Fine-tune',\n",
    "        save_path=f'best_model_fine-tune_{idx}.pth'\n",
    "    )\n",
    "    \n",
    "    # Load the best fine-tuned model\n",
    "    model.load_state_dict(torch.load(f'best_model_fine-tune_{idx}.pth'))\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc = validate_coral(\n",
    "        model, \n",
    "        val_loader, \n",
    "        lambda logits, labels: coral_loss(logits, labels, class_weights_tensor)\n",
    "    )\n",
    "    print(f\"Combination {idx+1}: Final Val Loss: {val_loss:.4f}, Final Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'LR_INITIAL': lr_initial,\n",
    "        'LR_FINE_TUNE': lr_fine_tune,\n",
    "        'ACTIVATE_WEIGHTS': activate_weights,\n",
    "        'DROPOUT_PROB': dropout_prob,\n",
    "        'PATIENCE': patience,\n",
    "        'EPOCHS': epochs,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('hyperparameter_tuning_results.csv', index=False)\n",
    "\n",
    "# Find and display the best combination\n",
    "best_idx = results_df['val_acc'].idxmax()\n",
    "best_combination = results_df.iloc[best_idx]\n",
    "print(f\"\\nBest Combination Found:\")\n",
    "print(f\"LR_INITIAL: {best_combination['LR_INITIAL']}\")\n",
    "print(f\"LR_FINE_TUNE: {best_combination['LR_FINE_TUNE']}\")\n",
    "print(f\"ACTIVATE_WEIGHTS: {best_combination['ACTIVATE_WEIGHTS']}\")\n",
    "print(f\"DROPOUT_PROB: {best_combination['DROPOUT_PROB']}\")\n",
    "print(f\"PATIENCE: {best_combination['PATIENCE']}\")\n",
    "print(f\"EPOCHS: {best_combination['EPOCHS']}\")\n",
    "print(f\"Validation Accuracy: {best_combination['val_acc']:.4f}\")\n",
    "print(f\"Validation Loss: {best_combination['val_loss']:.4f}\")\n",
    "\n",
    "# Cell 10: Prediction Function (Optional)\n",
    "def predict_grade_coral(img_path, model, le, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).int()\n",
    "        pred_level = torch.sum(preds).item()\n",
    "        if pred_level == 0:\n",
    "            return le.inverse_transform([0])[0]\n",
    "        elif pred_level == num_classes - 1:\n",
    "            return le.inverse_transform([num_classes - 1])[0]\n",
    "        else:\n",
    "            return le.inverse_transform([pred_level])[0]\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "# sample_img_path = val_df.iloc[0]['full_path']\n",
    "# predicted_grade = predict_grade_coral(sample_img_path, model, le, val_transform)\n",
    "# print(f\"Predicted grade for {sample_img_path}: {predicted_grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the gpu for inference\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# show a progress bar for inference\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# infer on only the validation set\n",
    "val_df['predicted'] = val_df['filename'].progress_apply(lambda x: predict_grade_coral(x, model.to(device), le, val_transform))\n",
    "val_df['correct'] = val_df['predicted'] == val_df['grade']\n",
    "print(f\"Overall Validation Set Accuracy: {val_df['correct'].mean():.4f}\")\n",
    "# print accuracy by grade, sorted by accuracy, accuracy in percent\n",
    "print(val_df.groupby('grade')['correct'].mean().sort_values(ascending=False) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (assumes GPU like H200 is available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load dataset\n",
    "big_df = pd.read_csv('../scrape/psa_sales4.csv')  # Replace with your CSV path\n",
    "image_dir = '../scrape/cropped4'  # Replace with your image directory\n",
    "big_df['filename'] = big_df['certNumber'].apply(lambda x: os.path.join(image_dir, f\"cert_{x}.jpg\"))\n",
    "\n",
    "# Print how many images are missing\n",
    "print(f\"Missing images: {len(big_df[big_df['filename'].apply(lambda x: not os.path.exists(x))])}\")\n",
    "\n",
    "# Remove non-existing images\n",
    "big_df = big_df[big_df['filename'].apply(os.path.exists)]\n",
    "\n",
    "# drop grades that are not in the original dataset\n",
    "big_df = big_df[big_df['grade'].isin(df['grade'].unique())]\n",
    "print(f\"Dataset size after filtering: {len(big_df)}\")\n",
    "\n",
    "# show a progress bar for inference\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "SPLIT_FRAC = 0.8\n",
    "\n",
    "# Split into training and validation sets\n",
    "big_train_df = big_df.sample(frac=SPLIT_FRAC, random_state=42)\n",
    "big_val_df = big_df.drop(big_train_df.index)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "big_train_df['label'] = le.transform(big_train_df['grade'])\n",
    "big_val_df['label'] = le.transform(big_val_df['grade'])\n",
    "\n",
    "# infer on only the validation set\n",
    "big_val_df['predicted'] = big_val_df['filename'].progress_apply(lambda x: predict_grade_coral(x, model.to(device), le, val_transform))\n",
    "big_val_df['correct'] = big_val_df['predicted'] == big_val_df['grade']\n",
    "print(f\"Overall Validation Set Accuracy: {big_val_df['correct'].mean():.4f}\")\n",
    "# print accuracy by grade, sorted by accuracy, accuracy in percent\n",
    "print(big_val_df.groupby('grade')['correct'].mean().sort_values(ascending=False) * 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

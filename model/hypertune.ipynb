{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search routine for repeated training with altered CONSTANTS\n",
    "# (Assumes previous code with train_model, validate, data loaders, etc. is available)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "patience_list = [10, 7]                   # For example, test with patience=10 and 7\n",
    "epochs_initial_list = [50, 60]            # Try different numbers for the initial training phase\n",
    "epochs_fine_tune_list = [50, 60]          # Try different numbers for the fine-tuning phase\n",
    "\n",
    "# Variables to store the best model information\n",
    "best_val_acc_overall = 0.0\n",
    "best_hyperparams = {}\n",
    "best_model_overall_path = 'best_model_overall.pth'\n",
    "\n",
    "# Loop over all combinations of hyperparameters\n",
    "for patience in patience_list:\n",
    "    PATIENCE = patience  # Update the global constant for patience\n",
    "    for init_epochs in epochs_initial_list:\n",
    "        EPOCHS_INITIAL = init_epochs  # Update the global constant for initial epochs\n",
    "        for fine_tune_epochs in epochs_fine_tune_list:\n",
    "            EPOCHS_FINE_TUNE = fine_tune_epochs  # Update the global constant for fine-tuning epochs\n",
    "            \n",
    "            print(f\"\\nTraining with PATIENCE={PATIENCE}, EPOCHS_INITIAL={EPOCHS_INITIAL}, EPOCHS_FINE_TUNE={EPOCHS_FINE_TUNE}\")\n",
    "            \n",
    "            # ------------------------------------------------------------------\n",
    "            # 1) Build a fresh model for this hyperparameter configuration\n",
    "            # ------------------------------------------------------------------\n",
    "            # model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            model = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
    "            # model = models.convnext_large(weights=models.ConvNeXt_Large_Weights.DEFAULT)\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(DROPOUT_PROB),\n",
    "                nn.Linear(1024, len(le.classes_))\n",
    "            )\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Freeze all layers except fc for initial training\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # ------------------------------------------------------------------\n",
    "            # 2) Set up loss, optimizer, and scheduler for initial training\n",
    "            # ------------------------------------------------------------------\n",
    "            if ACTIVATE_WEIGHTS_TENSOR:\n",
    "                criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "            else:\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_INITIAL, eta_min=1e-6)\n",
    "            \n",
    "            # Train the model on the initial phase\n",
    "            history_initial = train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                          scheduler, num_epochs=EPOCHS_INITIAL, phase='Initial')\n",
    "            \n",
    "            # Load best initial-phase model\n",
    "            model.load_state_dict(torch.load('best_model_initial.pth'))\n",
    "            \n",
    "            # ------------------------------------------------------------------\n",
    "            # 3) Fine-tuning: Unfreeze additional layers (layer3, layer4, and fc)\n",
    "            # ------------------------------------------------------------------\n",
    "            for name, param in model.named_parameters():\n",
    "                if any(layer in name for layer in ['layer3', 'layer4', 'fc']):\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Re-initialize the optimizer and scheduler for fine-tuning\n",
    "            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FINE_TUNE, eta_min=1e-6)\n",
    "            \n",
    "            # Train the model on the fine-tuning phase\n",
    "            history_fine = train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                       scheduler, num_epochs=EPOCHS_FINE_TUNE, phase='Fine-tune')\n",
    "            \n",
    "            # Load best fine-tuned model\n",
    "            model.load_state_dict(torch.load('best_model_fine-tune.pth'))\n",
    "            \n",
    "            # ------------------------------------------------------------------\n",
    "            # 4) Validate final model and update the best model if applicable\n",
    "            # ------------------------------------------------------------------\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "            print(f\"Hyperparams: PATIENCE={PATIENCE}, EPOCHS_INITIAL={EPOCHS_INITIAL}, EPOCHS_FINE_TUNE={EPOCHS_FINE_TUNE}\")\n",
    "            print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > best_val_acc_overall:\n",
    "                best_val_acc_overall = val_acc\n",
    "                best_hyperparams = {'PATIENCE': PATIENCE, 'EPOCHS_INITIAL': EPOCHS_INITIAL, 'EPOCHS_FINE_TUNE': EPOCHS_FINE_TUNE}\n",
    "                torch.save(model.state_dict(), best_model_overall_path)\n",
    "                print(\"==> New best model saved!\")\n",
    "                \n",
    "print(\"\\nHyperparameter search completed!\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "print(f\"Best Overall Validation Accuracy: {best_val_acc_overall:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Load the best model for inference and print overall accuracy\n",
    "# ------------------------------------------------------------------\n",
    "model.load_state_dict(torch.load(best_model_overall_path))\n",
    "print(\"Best model loaded for inference.\")\n",
    "\n",
    "# Optionally, run a final validation to print the accuracy again\n",
    "final_val_loss, final_val_acc = validate(model, val_loader, criterion)\n",
    "print(f\"Final Model Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Now you can use the 'predict_grade' function with this best model for inference.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

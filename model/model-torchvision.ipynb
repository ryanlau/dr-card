{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and TorchVision if not already installed\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set device (assumes GPU like your H200 is available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('../scrape/psa_sales_190786_20250222_143954.csv')  # Replace with your CSV file path\n",
    "image_dir = '../scrape/pictures'  # Replace with your image directory path\n",
    "df['filename'] = df['filename'].apply(lambda x: os.path.join(image_dir, x))\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "# Encode the grade labels (e.g., 'PSA1' to 'PSA10') into integers\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "train_df['label'] = le.transform(train_df['grade'])\n",
    "val_df['label'] = le.transform(val_df['grade'])\n",
    "\n",
    "# Optional: Compute class weights to handle imbalance\n",
    "classes = np.unique(train_df['grade'])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['grade'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define transforms for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define custom dataset class\n",
    "class CardDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filename']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 64  # Adjust based on your H200 GPU memory\n",
    "train_dataset = CardDataset(train_df, transform=train_transform)\n",
    "val_dataset = CardDataset(val_df, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Build the model with ResNet101\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "model = resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1024, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers except the final classifier (fc)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # Remove weight if not needed\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Define validation function\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Training loop with early stopping (initial phase)\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "# Load best model and unfreeze layer4 and fc for fine-tuning\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "# Reset for fine-tuning\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history_fine = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    history_fine['train_loss'].append(train_loss)\n",
    "    history_fine['val_loss'].append(val_loss)\n",
    "    history_fine['val_accuracy'].append(val_acc)\n",
    "    \n",
    "    print(f'Fine-tune Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_finetune.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "# Load the best fine-tuned model\n",
    "model.load_state_dict(torch.load('best_model_finetune.pth'))\n",
    "\n",
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['val_accuracy'] + history_fine['val_accuracy'], label='Val Accuracy')\n",
    "plt.plot(history['train_loss'] + history_fine['train_loss'], label='Train Loss')  # Proxy since train acc not computed\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_loss'] + history_fine['val_loss'], label='Val Loss')\n",
    "plt.plot(history['train_loss'] + history_fine['train_loss'], label='Train Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'card_grader_model.pth')\n",
    "\n",
    "# Define prediction function\n",
    "def predict_grade(img_path, model, le, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    predicted_class = predicted.item()\n",
    "    predicted_grade = le.inverse_transform([predicted_class])[0]\n",
    "    return predicted_grade\n",
    "\n",
    "# Example prediction\n",
    "new_card_path = '../scrape/pictures/cert_01443963.jpg'  # Replace with your test image path\n",
    "predicted_grade = predict_grade(new_card_path, model, le, val_transform) # (expected 8)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

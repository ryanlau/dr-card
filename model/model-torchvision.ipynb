{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision opencv-python pandas scikit-learn pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm best_model_fine-tune.pth\n",
    "!rm best_model_initial.pth\n",
    "!rm card_grader_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "ACTIVATE_WEIGHTS_TENSOR = 0\n",
    "ACTIVATE_CROPPING = 0\n",
    "\n",
    "# Set device (assumes GPU like H200 is available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../scrape/psa_sales_190786_20250222_143954.csv')  # Replace with your CSV path\n",
    "image_dir = '../scrape/pictures'  # Replace with your image directory\n",
    "df['filename'] = df['certNumber'].apply(lambda x: os.path.join(image_dir, f\"cert_{x}.jpg\"))\n",
    "\n",
    "# print how many images are missing\n",
    "print(f\"Missing images: {len(df[df['filename'].apply(lambda x: not os.path.exists(x))])}\")\n",
    "\n",
    "# Remove non-existing images\n",
    "df = df[df['filename'].apply(os.path.exists)]\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "train_df['label'] = le.transform(train_df['grade'])\n",
    "val_df['label'] = le.transform(val_df['grade'])\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Training set grade distribution:\\n\", train_df['grade'].value_counts())\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "if ACTIVATE_WEIGHTS_TENSOR:\n",
    "    classes = np.unique(train_df['grade'])\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=train_df['grade'])\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define cropping functions inspired by psa_pokemon_cards\n",
    "def crop_card_for_light_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, otsu_grad = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(otsu_grad, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    height, width = image.shape[:2]\n",
    "    image_area = height * width\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    for contour in contours:\n",
    "        peri = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, 0.001 * peri, True)\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        area = w * h\n",
    "        if 0.48 * image_area <= area <= 0.6 * image_area:\n",
    "            return image[y:y+h, x:x+w]\n",
    "    return None\n",
    "\n",
    "def crop_card_for_dark_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), -10)\n",
    "    adaptive_binary = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 7, 3)\n",
    "    edges = cv2.Canny(adaptive_binary, 100, 200)\n",
    "    binarized_grad = 255 - edges\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "    open_binarized_grad = cv2.morphologyEx(binarized_grad, cv2.MORPH_OPEN, kernel)\n",
    "    contours, _ = cv2.findContours(open_binarized_grad, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    height, width = image.shape[:2]\n",
    "    image_area = height * width\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if 0.48 * image_area <= area <= 0.7 * image_area:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            return image[y:y+h, x:x+w]\n",
    "    return None\n",
    "\n",
    "def crop_card(image):\n",
    "    cropped = crop_card_for_light_image(image)\n",
    "    if cropped is not None:\n",
    "        return cropped\n",
    "    cropped = crop_card_for_dark_image(image)\n",
    "    if cropped is not None:\n",
    "        return cropped\n",
    "    return image  # Fallback to original if cropping fails\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Custom dataset with cropping\n",
    "class CardDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        if ACTIVATE_CROPPING:\n",
    "            self.failed_crops = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filename']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if ACTIVATE_CROPPING:\n",
    "            image_np = np.array(image)\n",
    "            cropped_np = crop_card(image_np)\n",
    "            if cropped_np is image_np:  # Cropping failed, log it\n",
    "                self.failed_crops.append(img_path)\n",
    "            image = Image.fromarray(cropped_np)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# print some sample cropped images\n",
    "def show_sample_crops(df, transform):\n",
    "    sample_df = df.sample(5)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, (_, row) in zip(axes, sample_df.iterrows()):\n",
    "        img_path = row['filename']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        cropped_np = crop_card(image_np)\n",
    "        image = Image.fromarray(cropped_np)\n",
    "        if transform:\n",
    "            image = transform(image)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if ACTIVATE_CROPPING:\n",
    "    show_sample_crops(train_df, train_transform)\n",
    "else:\n",
    "    print(\"Skipping cropping visualization.\")\n",
    "    # instead show some sample images\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for ax, (_, row) in zip(axes, train_df.sample(5).iterrows()):\n",
    "        img_path = row['filename']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 128  # Adjust based on GPU memory\n",
    "train_dataset = CardDataset(train_df, transform=train_transform)\n",
    "val_dataset = CardDataset(val_df, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Log failed crops\n",
    "if ACTIVATE_CROPPING:\n",
    "    print(f\"Training failed crops: {len(train_dataset.failed_crops)}\")\n",
    "    print(f\"Validation failed crops: {len(val_dataset.failed_crops)}\")\n",
    "\n",
    "# Build ResNet50 model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.Linear(1024, len(le.classes_))\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers except fc initially\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Loss and optimizer\n",
    "if ACTIVATE_WEIGHTS_TENSOR:\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, phase='initial'):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    scaler = GradScaler()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print(f'{phase} Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_{phase.lower()}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "# Initial training\n",
    "history_initial = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 50, 'Initial')\n",
    "\n",
    "# Load best model and unfreeze layer4 and fc for fine-tuning\n",
    "model.load_state_dict(torch.load('best_model_initial.pth'))\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "# Fine-tuning\n",
    "history_fine = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 50, 'Fine-tune')\n",
    "\n",
    "# Load best fine-tuned model\n",
    "model.load_state_dict(torch.load('best_model_fine-tune.pth'))\n",
    "\n",
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_initial['val_accuracy'] + history_fine['val_accuracy'], label='Val Accuracy')\n",
    "plt.plot(history_initial['train_loss'] + history_fine['train_loss'], label='Train Loss')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_initial['val_loss'] + history_fine['val_loss'], label='Val Loss')\n",
    "plt.plot(history_initial['train_loss'] + history_fine['train_loss'], label='Train Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'card_grader_model.pth')\n",
    "\n",
    "# Prediction function\n",
    "def predict_grade(img_path, model, le, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    cropped_np = crop_card(image_np)\n",
    "    image = Image.fromarray(cropped_np)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return le.inverse_transform([predicted.item()])[0]\n",
    "\n",
    "# Test prediction\n",
    "test_img_path = '../scrape/pictures/cert_01443963.jpg'  # Replace with your test image\n",
    "predicted_grade = predict_grade(test_img_path, model, le, val_transform)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = '../scrape/pictures/test.jpg'  # Replace with your test image\n",
    "predicted_grade = predict_grade(test_img_path, model, le, val_transform)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}; Expected 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = '../scrape/pictures/cert_99449754.jpg'  # Replace with your test image\n",
    "predicted_grade = predict_grade(test_img_path, model, le, val_transform)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}; Expected 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = '../scrape/pictures/cert_95743446.jpg'  # Replace with your test image\n",
    "predicted_grade = predict_grade(test_img_path, model, le, val_transform)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}; Expected 6\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

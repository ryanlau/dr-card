{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture (UNet) \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder1 = DoubleConv(3, 64)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.decoder3 = DoubleConv(512 + 256, 256)\n",
    "        self.decoder2 = DoubleConv(256 + 128, 128)\n",
    "        self.decoder1 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.decoder3(torch.cat([self.upsample(e4), e3], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([self.upsample(d3), e2], dim=1))\n",
    "        d1 = self.decoder1(torch.cat([self.upsample(d2), e1], dim=1))\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(d1))\n",
    "\n",
    "# Helper function: given a binary mask, get the bounding box coordinates\n",
    "def get_bounding_box(binary_mask):\n",
    "    # Find indices where mask is positive\n",
    "    coords = np.column_stack(np.where(binary_mask > 0))\n",
    "    if coords.size == 0:\n",
    "        return None\n",
    "    y_min, x_min = coords[:,0].min(), coords[:,1].min()\n",
    "    y_max, x_max = coords[:,0].max(), coords[:,1].max()\n",
    "    return x_min, y_min, x_max, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform used for inference (resize, normalize, tensor conversion)\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process an image, run inference, and crop the original image using the predicted mask.\n",
    "def process_and_crop_image(image_path, model, transform, device):\n",
    "    # Read original image\n",
    "    image_orig = cv2.imread(image_path)\n",
    "    if image_orig is None:\n",
    "        print(f\"Failed to read {image_path}\")\n",
    "        return None\n",
    "    # Convert from BGR to RGB\n",
    "    image_orig = cv2.cvtColor(image_orig, cv2.COLOR_BGR2RGB)\n",
    "    orig_h, orig_w, _ = image_orig.shape\n",
    "    \n",
    "    # Apply transform to prepare input for the model\n",
    "    transformed = transform(image=image_orig)\n",
    "    image_trans = transformed['image']  # Tensor shape: (3, 512, 512)\n",
    "    image_trans = image_trans.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(image_trans)\n",
    "    \n",
    "    # Squeeze batch dimension and convert to numpy\n",
    "    pred_np = pred.squeeze().cpu().numpy()  # shape: (512, 512)\n",
    "    \n",
    "    # Threshold to get binary mask\n",
    "    binary_mask = (pred_np > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Compute bounding box from mask\n",
    "    bbox = get_bounding_box(binary_mask)\n",
    "    if bbox is None:\n",
    "        print(f\"No mask detected for {image_path}. Skipping cropping.\")\n",
    "        return None\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    \n",
    "    # Map coordinates back to the original image dimensions\n",
    "    scale_x = orig_w / 512.0\n",
    "    scale_y = orig_h / 512.0\n",
    "    x_min_orig = int(x_min * scale_x)\n",
    "    y_min_orig = int(y_min * scale_y)\n",
    "    x_max_orig = int(x_max * scale_x)\n",
    "    y_max_orig = int(y_max * scale_y)\n",
    "    \n",
    "    # Crop the original image using the mapped bounding box\n",
    "    cropped_image = image_orig[y_min_orig:y_max_orig, x_min_orig:x_max_orig]\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "# Optional: function to display the cropped result\n",
    "def display_image(image, title=\"Image\"):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: load the trained model and prepare directories\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model and load trained weights\n",
    "model = UNet().to(device)\n",
    "model_weights = 'best_model.pth'\n",
    "if os.path.exists(model_weights):\n",
    "    checkpoint = torch.load(model_weights, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model weights from {model_weights}\")\n",
    "else:\n",
    "    print(f\"Model weights file {model_weights} not found. Please check the path.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_dir = 'pictures'\n",
    "output_dir = 'cropped'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each image in the 'pictures' directory and save the cropped version\n",
    "supported_ext = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "image_files = [f for f in os.listdir(input_dir) if os.path.splitext(f)[1].lower() in supported_ext]\n",
    "\n",
    "print(f\"Found {len(image_files)} image(s) in '{input_dir}' directory.\")\n",
    "\n",
    "for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    cropped = process_and_crop_image(input_path, model, transform, device)\n",
    "    if cropped is not None:\n",
    "        # Convert RGB back to BGR for saving via OpenCV\n",
    "        cropped_bgr = cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cropped_bgr)\n",
    "        # Optional: display the cropped image\n",
    "        # display_image(cropped, title=filename)\n",
    "    else:\n",
    "        print(f\"Skipping {filename} due to no detected mask.\")\n",
    "\n",
    "print(f\"Cropped images have been saved to the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'my_image.jpg' with the name of your image in the \"pictures\" directory\n",
    "input_filename = \"test2.jpg\"\n",
    "# input_path = os.path.join(\"pictures\", input_filename)\n",
    "\n",
    "# Process the image to obtain the cropped output\n",
    "cropped = process_and_crop_image(input_filename, model, transform, device)\n",
    "if cropped is not None:\n",
    "    # Construct the output filename (e.g., \"my_image_cropped.jpg\")\n",
    "    filename, ext = os.path.splitext(input_filename)\n",
    "    output_filename = filename + \"_cropped\" + ext\n",
    "    output_path = os.path.join(\"cropped\", output_filename)\n",
    "    \n",
    "    # Ensure the \"cropped\" directory exists\n",
    "    os.makedirs(\"cropped\", exist_ok=True)\n",
    "    \n",
    "    # Convert cropped image from RGB (used in processing) to BGR (for OpenCV saving)\n",
    "    cropped_bgr = cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, cropped_bgr)\n",
    "    print(f\"Cropped image saved as {output_path}\")\n",
    "else:\n",
    "    print(\"No mask detected. Image was not cropped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import os\n",
    "import timm\n",
    "from torchvision.transforms import RandAugment, RandomErasing\n",
    "import re\n",
    "\n",
    "# Training combination 28/32: LR_INITIAL=0.0005, LR_FINE_TUNE=5e-05, ACTIVATE_WEIGHTS=0, DROPOUT_PROB=0.3, PATIENCE=15, EPOCHS=50\n",
    "\n",
    "# Hyperparameters\n",
    "ACTIVATE_WEIGHTS_TENSOR = 0  # Set to 1 to use class weights, 0 to disable\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_INITIAL = 50\n",
    "EPOCHS_FINE_TUNE = 100\n",
    "DROPOUT_PROB = 0.3\n",
    "LR_INITIAL = 0.0005\n",
    "# LR_FINE_TUNE = 5e-5\n",
    "LR_FINE_TUNE = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 7\n",
    "SPLIT_FRAC = 0.8  # Fraction of data for training (rest for validation)\n",
    "\n",
    "# AI_MODEL='convnext_large_mlp.clip_laion2b_soup_ft_in12k_384'\n",
    "# AI_MODEL='convnext_xxlarge.clip_laion2b_soup_ft_in12k'\n",
    "# AI_MODEL=\"convnextv2_nano.fcmae\"\n",
    "AI_MODEL=\"convnext_base\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Load and Preprocess Dataset\n",
    "# Load the CSV file and prepare image paths\n",
    "# df = pd.read_csv('../scrape/psa_sales4.csv')\n",
    "# image_dir = '../scrape/cropped4'\n",
    "# df['full_path'] = df['certNumber'].apply(lambda x: os.path.join(image_dir, f\"cert_{x}.jpg\"))\n",
    "\n",
    "image_dir = '../grade_comparisons/'  # <-- Update this path as needed\n",
    "\n",
    "# List all files in the directory that are JPEG images\n",
    "file_names = [\n",
    "    f for f in os.listdir(image_dir)\n",
    "    if os.path.isfile(os.path.join(image_dir, f)) and f.lower().endswith('.jpg')\n",
    "]\n",
    "\n",
    "def extract_grade(full_path):\n",
    "    # This regex looks for the pattern \"cropped_{grade}_cert\"\n",
    "    pattern = r'cropped_(\\d+)_cert'\n",
    "    match = re.search(pattern, full_path)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        # If the full_path doesn't match the pattern, print a message and return None\n",
    "        print(\"no match for\", full_path)\n",
    "        return None\n",
    "\n",
    "# Create a DataFrame with the full_paths\n",
    "df = pd.DataFrame({'full_path': file_names})\n",
    "\n",
    "# Apply the extraction function to create a new 'grade' column\n",
    "df['grade'] = df['full_path'].apply(extract_grade)\n",
    "\n",
    "# Create a column for the full file path\n",
    "df['full_path'] = df['full_path'].apply(lambda x: os.path.join(image_dir, x))\n",
    "\n",
    "# Check for missing images by using the full path\n",
    "missing_count = len(df[df['full_path'].apply(lambda x: not os.path.exists(x))])\n",
    "print(f\"Missing images: {missing_count}\")\n",
    "\n",
    "# Filter out missing images based on the full path\n",
    "df = df[df['full_path'].apply(os.path.exists)]\n",
    "print(f\"Dataset size after filtering: {len(df)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Unique grades in full dataset:\", df['grade'].unique())\n",
    "print(\"Number of unique grades in full dataset:\", df['grade'].nunique())\n",
    "\n",
    "grade_counts = df['grade'].value_counts()\n",
    "\n",
    "# also add percent of total to grade_counts as a new column\n",
    "grade_counts = grade_counts.reset_index()\n",
    "grade_counts.columns = ['grade', 'count']\n",
    "grade_counts['percent'] = grade_counts['count'] / len(df) * 100\n",
    "\n",
    "print(grade_counts)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=SPLIT_FRAC, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Encode labels (0 to 18 for grades 1 to 10 with half-point increments)\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "train_df['label'] = le.transform(train_df['grade'])\n",
    "val_df['label'] = le.transform(val_df['grade'])\n",
    "num_classes = len(le.classes_)  # Should be 19\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Compute class weights to handle imbalance (optional)\n",
    "if ACTIVATE_WEIGHTS_TENSOR:\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    print(\"Class weights computed and moved to device\")\n",
    "else:\n",
    "    class_weights_tensor = None\n",
    "    print(\"Class weights disabled\")\n",
    "\n",
    "# Cell 3: Define Transforms\n",
    "# Custom padding function to make images square\n",
    "def pad_to_square(image, fill=0, padding_mode=\"constant\"):\n",
    "    w, h = image.size\n",
    "    max_wh = max(w, h)\n",
    "    pad_left = (max_wh - w) // 2\n",
    "    pad_top = (max_wh - h) // 2\n",
    "    pad_right = max_wh - w - pad_left\n",
    "    pad_bottom = max_wh - h - pad_top\n",
    "    return transforms.Pad((pad_left, pad_top, pad_right, pad_bottom), fill=fill, padding_mode=padding_mode)(image)\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: pad_to_square(img, fill=0)),\n",
    "    transforms.Resize(224),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: pad_to_square(img, fill=0)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Cell 4: Create Dataset and DataLoader\n",
    "# Custom Dataset class\n",
    "class CardDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['full_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = CardDataset(train_df, transform=train_transform)\n",
    "val_dataset = CardDataset(val_df, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(\"Data loaders created\")\n",
    "\n",
    "# Cell 5: Define CORAL Head and Modify Model\n",
    "# Custom CORAL head for ordinal regression\n",
    "class CoralHead(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes - 1)  # K-1 logits for CORAL\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load ConvNeXt model and replace the head with CORAL head\n",
    "model = timm.create_model(AI_MODEL, pretrained=True)\n",
    "in_features = model.head.fc.in_features\n",
    "model.head.fc = CoralHead(in_features, num_classes)\n",
    "model = model.to(device)\n",
    "print(\"Model with CORAL head initialized\")\n",
    "\n",
    "# Cell 6: Define CORAL Loss Function\n",
    "def coral_loss(logits, levels, class_weights=None):\n",
    "    \"\"\"\n",
    "    Compute CORAL loss as the sum of binary cross-entropy losses for each threshold.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape (batch_size, K-1) - model outputs\n",
    "        levels: Tensor of shape (batch_size,) - true labels (0 to K-1)\n",
    "        class_weights: Tensor of shape (K,) - optional class weights\n",
    "    \"\"\"\n",
    "    batch_size = logits.size(0)\n",
    "    levels = levels.view(-1, 1).to(device)\n",
    "    # Create target matrix: 1 if true label > threshold, 0 otherwise\n",
    "    targets = (levels > torch.arange(num_classes - 1).to(device)).float()\n",
    "    # Compute binary cross-entropy for each threshold\n",
    "    loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    # Apply class weights if provided\n",
    "    if class_weights is not None:\n",
    "        sample_weights = class_weights[levels.squeeze()].to(device)\n",
    "        loss = loss * sample_weights.view(-1, 1)\n",
    "    return loss.mean()\n",
    "\n",
    "# Cell 7: Define Validation Function\n",
    "def validate_coral(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            # Predict grade by summing binary decisions\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).int()\n",
    "            pred_levels = torch.sum(preds, dim=1).cpu().numpy()\n",
    "            true_levels = labels.cpu().numpy()\n",
    "            correct += np.sum(pred_levels == true_levels)\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Cell 8: Define Training Function\n",
    "def train_model_coral(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, phase='initial'):\n",
    "    best_val_loss = float('inf')\n",
    "    best_overfit_gap = -float('inf')  # Initialize the max gap (val_loss - train_loss)\n",
    "    patience_counter = 0\n",
    "    scaler = GradScaler()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        val_loss, val_acc = validate_coral(model, val_loader, criterion)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        print(f'{phase} Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_{phase.lower()}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "        \n",
    "        # Compute the overfit gap (higher gap means more overfitting)\n",
    "        overfit_gap = val_loss - train_loss\n",
    "        if overfit_gap > best_overfit_gap:\n",
    "            best_overfit_gap = overfit_gap\n",
    "            torch.save(model.state_dict(), 'OVERFIT_model.pth')\n",
    "            print(f\"Epoch {epoch+1}: Overfit gap {overfit_gap:.4f} (new maximum), saving OVERFIT_model.pth\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Cell 9: Initial Training (Frozen Backbone)\n",
    "# Freeze all layers except the head\n",
    "for name, param in model.named_parameters():\n",
    "    if 'head' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_INITIAL, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_INITIAL, eta_min=1e-6)\n",
    "\n",
    "# Train the model (initial phase)\n",
    "print(\"Starting initial training...\")\n",
    "history_initial = train_model_coral(\n",
    "    model, train_loader, val_loader,\n",
    "    lambda logits, labels: coral_loss(logits, labels, class_weights_tensor),\n",
    "    optimizer, scheduler, EPOCHS_INITIAL, 'Initial'\n",
    ")\n",
    "\n",
    "# Cell 10: Fine-Tuning (Unfreeze All Layers)\n",
    "# Load the best model from initial training\n",
    "model.load_state_dict(torch.load('best_model_initial.pth'))\n",
    "print(\"Loaded best model from initial training\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Define optimizer and scheduler for fine-tuning\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_FINE_TUNE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FINE_TUNE, eta_min=1e-6)\n",
    "\n",
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "history_fine = train_model_coral(\n",
    "    model, train_loader, val_loader,\n",
    "    lambda logits, labels: coral_loss(logits, labels, class_weights_tensor),\n",
    "    optimizer, scheduler, EPOCHS_FINE_TUNE, 'Fine-tune'\n",
    ")\n",
    "\n",
    "# Cell 11: Final Evaluation and Save Model\n",
    "# Load the best fine-tuned model\n",
    "model.load_state_dict(torch.load('best_model_fine-tune.pth'))\n",
    "print(\"Loaded best fine-tuned model\")\n",
    "\n",
    "# Final evaluation\n",
    "val_loss, val_acc = validate_coral(model, val_loader, lambda logits, labels: coral_loss(logits, labels, class_weights_tensor))\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}, Final Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'card_grader_model_coral.pth')\n",
    "print(\"Final model saved as 'card_grader_model_coral.pth'\")\n",
    "\n",
    "# Cell 12: Prediction Function\n",
    "def predict_grade_coral(img_path, model, le, transform):\n",
    "    \"\"\"\n",
    "    Predict the grade of a card image using the CORAL model.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to the image file\n",
    "        model: Trained CORAL model\n",
    "        le: LabelEncoder instance\n",
    "        transform: Image transformation pipeline\n",
    "    Returns:\n",
    "        Predicted grade as a string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).int()\n",
    "        pred_level = torch.sum(preds).item()\n",
    "        if pred_level == 0:\n",
    "            return le.inverse_transform([0])[0]\n",
    "        elif pred_level == num_classes - 1:\n",
    "            return le.inverse_transform([num_classes - 1])[0]\n",
    "        else:\n",
    "            return le.inverse_transform([pred_level])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Plot Training Metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine the histories from the initial training and fine-tuning phases\n",
    "combined_train_loss = history_initial['train_loss'] + history_fine['train_loss']\n",
    "combined_val_loss = history_initial['val_loss'] + history_fine['val_loss']\n",
    "combined_val_accuracy = history_initial['val_accuracy'] + history_fine['val_accuracy']\n",
    "\n",
    "# Create an array of epoch numbers\n",
    "epochs = np.arange(1, len(combined_train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, combined_train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, combined_val_loss, label='Validation Loss', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, combined_val_accuracy, label='Validation Accuracy', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the gpu for inference\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "# TODO: BEST MODEL PROBABLY NOT LOADED!!!\n",
    "# model.load_state_dict(torch.load(f'best_model_fine-tune_{idx}.pth'))\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# load good model\n",
    "torch.save(model.state_dict(), 'card_grader_model_coral.pth')\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# show a progress bar for inference\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# infer on only the validation set\n",
    "val_df['predicted'] = val_df['full_path'].progress_apply(lambda x: predict_grade_coral(x, model.to(device), le, val_transform))\n",
    "val_df['correct'] = val_df['predicted'] == val_df['grade']\n",
    "print(f\"Overall Validation Set Accuracy: {val_df['correct'].mean():.4f}\")\n",
    "# print accuracy by grade, sorted by accuracy, accuracy in percent\n",
    "print(val_df.groupby('grade')['correct'].mean().sort_values(ascending=False) * 100)\n",
    "\n",
    "# for fun infer on the training set too\n",
    "train_df['predicted'] = train_df['full_path'].progress_apply(lambda x: predict_grade_coral(x, model.to(device), le, val_transform))\n",
    "train_df['correct'] = train_df['predicted'] == train_df['grade']\n",
    "print(f\"Overall Training Set Accuracy: {train_df['correct'].mean():.4f}\")\n",
    "# print accuracy by grade, sorted by accuracy, accuracy in percent\n",
    "print(train_df.groupby('grade')['correct'].mean().sort_values(ascending=False) * 100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

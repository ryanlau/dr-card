{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable mixed precision to optimize H200 GPU performance\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('path/to/card_data.csv')  # Replace with your CSV file path\n",
    "image_dir = 'path/to/images'  # Replace with your image directory path\n",
    "df['filename'] = df['filename'].apply(lambda x: os.path.join(image_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the grade labels (e.g., 'PSA1' to 'PSA10') into integers\n",
    "le = LabelEncoder()\n",
    "le.fit(df['grade'])\n",
    "train_labels = le.transform(train_df['grade'])\n",
    "val_labels = le.transform(val_df['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compute class weights to handle imbalance (remove if not needed)\n",
    "classes = np.unique(train_df['grade'])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['grade'])\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "train_sample_weights = [class_weights_dict[label] for label in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with sample weights\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['filename'], train_labels, train_sample_weights))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df['filename'], val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image loading and preprocessing function\n",
    "def load_and_preprocess_image(filename, label, weight=None):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])  # Resizing to 224x224 as requested\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode for 10 classes\n",
    "    if weight is not None:\n",
    "        return img, label, weight\n",
    "    else:\n",
    "        return img, label\n",
    "\n",
    "# Define data augmentation function for training\n",
    "def augment_image(image, label, weight=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    # Add more augmentations here if desired (e.g., random rotation)\n",
    "    if weight is not None:\n",
    "        return image, label, weight\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure datasets for performance\n",
    "batch_size = 64  # Increased from 32; adjust to 128 or higher if H200 memory allows\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y, w: load_and_preprocess_image(x, y, w),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "train_dataset = train_dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    load_and_preprocess_image,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build the model with ResNet101\n",
    "base_model = ResNet101(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax', dtype='float32')  # Output in float32 for stability\n",
    "])\n",
    "\n",
    "# Freeze the base model initially\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (initial phase)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Fine-tune by unfreezing the last 10 layers\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Lower learning rate for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'] + history_fine.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'] + history_fine.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'] + history_fine.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'] + history_fine.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model.save('card_grader_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict_grade(img_path, model, le):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = img / 255.0\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_grade = le.inverse_transform([predicted_class])[0]\n",
    "    return predicted_grade\n",
    "\n",
    "# Example prediction\n",
    "new_card_path = 'path/to/new/card.jpg'  # Replace with your test image path\n",
    "predicted_grade = predict_grade(new_card_path, model, le)\n",
    "print(f\"Predicted PSA grade: {predicted_grade}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
